{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/","title":"\ud83e\uddca Implementa\u00e7\u00e3o Data Lakehouse","text":"<p>Este projeto demonstra uma arquitetura de Data Lakehouse local, utilizando ferramentas open-source de ponta.</p>"},{"location":"docs/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Criar uma estrutura funcional para experimentar o modelo Lakehouse com:</p> <ul> <li>Processamento distribu\u00eddo com Spark</li> <li>Gerenciamento de tabelas com Apache Iceberg</li> <li>Armazenamento com MinIO (S3 compatible)</li> <li>Ambiente interativo com Jupyter Lab</li> </ul>"},{"location":"docs/#destaques","title":"\ud83d\udccc Destaques","text":"<p>Totalmente containerizado</p> <p>Toda a stack roda via Docker, sem precisar instalar nada manualmente.</p> <p>Ideal para estudo e testes</p> <p>\u00c9 poss\u00edvel modificar os notebooks, os scripts de ingest\u00e3o e os dados sem quebrar a estrutura.</p>    Autores: Luiz Henrique, Maria Silva"},{"location":"docs/apacheIceberg/","title":"Apache Iceberg com PySpark","text":""},{"location":"docs/apacheIceberg/#o-que-e-o-apache-iceberg","title":"O que \u00e9 o Apache Iceberg?","text":"<p>Apache Iceberg \u00e9 uma tabela de dados open source voltada para ambientes de big data. Ele permite que voc\u00ea trabalhe com grandes volumes de dados de forma escal\u00e1vel, segura e confi\u00e1vel, suportando opera\u00e7\u00f5es como <code>UPDATE</code>, <code>DELETE</code> e <code>MERGE</code>, que normalmente n\u00e3o s\u00e3o suportadas nativamente em tabelas baseadas em arquivos como Parquet.</p>"},{"location":"docs/apacheIceberg/#integracao-com-pyspark","title":"Integra\u00e7\u00e3o com PySpark","text":"<p>Abaixo est\u00e1 a explica\u00e7\u00e3o das configura\u00e7\u00f5es necess\u00e1rias para integrar o PySpark ao Iceberg:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Iceberg\") \\\n    .master(\"spark://spark-iceberg:7077\") \\\n    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9001\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"docs/apacheIceberg/#explicacao-das-configuracoes","title":"Explica\u00e7\u00e3o das configura\u00e7\u00f5es","text":"<p>appName: Nome do job Spark.</p> <p>master: Define o endpoint do cluster Spark.</p> <p>spark.sql.catalog.iceberg: Cria um cat\u00e1logo chamado iceberg usando SparkCatalog.</p> <p>type = hadoop: Define o tipo de cat\u00e1logo (pode ser hadoop, hive, entre outros).</p> <p>warehouse: Local onde os dados ser\u00e3o fisicamente armazenados. Neste caso, em um bucket S3.</p> <p>extensions: Extens\u00f5es do Iceberg que habilitam comandos SQL como UPDATE, DELETE, etc.</p>"},{"location":"docs/apacheIceberg/#comandos-suportados","title":"Comandos suportados","text":"<p>Com a integra\u00e7\u00e3o do Iceberg, podemos executar comandos avan\u00e7ados diretamente via Spark SQL, como:</p> <p>INSERT INTO: Inser\u00e7\u00e3o de dados</p> <p>UPDATE: Atualiza\u00e7\u00e3o de registros</p> <p>DELETE: Remo\u00e7\u00e3o de registros com base em condi\u00e7\u00f5es</p> <p>MERGE INTO: Mesclagem de dados (estilo upsert)</p>"},{"location":"docs/apacheIceberg/#exemplos","title":"Exemplos","text":"<pre><code>spark.sql(\"\"\"\n    UPDATE football.premier_league\n    SET homeScore = 3, awayScore = 1\n    WHERE `Home Team` = 'Fulham'\n      AND `Away Team` = 'Liverpool'\n      AND Date = '2022-08-06'\n\"\"\")\n</code></pre>"},{"location":"docs/apacheIceberg/#vantagens-do-apache-iceberg","title":"Vantagens do Apache Iceberg","text":"<p>Suporte a transa\u00e7\u00f5es ACID</p> <p>Opera\u00e7\u00f5es incrementais (leitura de altera\u00e7\u00f5es)</p> <p>Melhor gerenciamento de parti\u00e7\u00f5es</p> <p>Compat\u00edvel com v\u00e1rios motores: Spark, Flink, Trino, Hive</p>"},{"location":"docs/apacheIceberg/#quando-usar","title":"Quando Usar?","text":"<p>Iceberg \u00e9 ideal para projetos que lidam com grandes volumes de dados e precisam de:</p> <p>Governan\u00e7a de dados</p> <p>Suporte a atualiza\u00e7\u00f5es e remo\u00e7\u00f5es</p> <p>Evolu\u00e7\u00e3o de schema</p> <p>Alta performance em consultas anal\u00edticas</p>"},{"location":"docs/apacheSpark/","title":"Introdu\u00e7\u00e3o ao Apache Spark com Python","text":"<p>Apache Spark \u00e9 uma engine de processamento de dados distribu\u00edda e de c\u00f3digo aberto, projetada para processar grandes volumes de dados de forma r\u00e1pida e eficiente.</p>"},{"location":"docs/apacheSpark/#o-que-e-spark","title":"\ud83d\udd25 O que \u00e9 Spark?","text":"<p>Spark permite executar tarefas de an\u00e1lise de dados em cluster (v\u00e1rios computadores), sendo muito mais r\u00e1pido que abordagens tradicionais como MapReduce, gra\u00e7as ao uso de mem\u00f3ria (RAM) para o processamento.</p>"},{"location":"docs/apacheSpark/#pyspark","title":"\ud83d\udc0d PySpark","text":"<p>PySpark \u00e9 a interface do Apache Spark para a linguagem Python. Ela permite criar aplica\u00e7\u00f5es de big data utilizando a sintaxe familiar do Python.</p>"},{"location":"docs/apacheSpark/#principais-componentes","title":"Principais componentes:","text":"<ul> <li>RDD (Resilient Distributed Dataset): Estrutura de dados distribu\u00edda de baixo n\u00edvel.</li> <li>DataFrame: Tabela distribu\u00edda com colunas nomeadas, semelhante ao pandas.</li> <li>Spark SQL: Permite executar consultas SQL sobre os dados.</li> <li>MLlib: Biblioteca de machine learning distribu\u00eddo.</li> <li>GraphX: Para processamento de grafos (dispon\u00edvel apenas em Scala, mas pode ser usado indiretamente em Python).</li> </ul>"},{"location":"docs/apacheSpark/#exemplo-basico-com-pyspark","title":"\u2705 Exemplo b\u00e1sico com PySpark","text":"<p>```python from pyspark.sql import SparkSession</p>"},{"location":"docs/apacheSpark/#criacao-de-uma-sparksession","title":"Cria\u00e7\u00e3o de uma SparkSession","text":"<p>spark = SparkSession.builder \\     .appName(\"Exemplo\") \\     .getOrCreate()</p>"},{"location":"docs/apacheSpark/#criando-um-dataframe","title":"Criando um DataFrame","text":"<p>dados = [(\"Ana\", 30), (\"Bruno\", 25), (\"Carlos\", 40)] colunas = [\"Nome\", \"Idade\"] df = spark.createDataFrame(dados, colunas)</p> <p>df.show()</p>"},{"location":"docs/execucao/","title":"\ud83d\ude80 Execu\u00e7\u00e3o do Projeto","text":"<p>Ap\u00f3s subir os containers, acesse os seguintes servi\u00e7os:</p>"},{"location":"docs/execucao/#acessos","title":"\ud83d\udd17 Acessos","text":"<ul> <li> <p>Jupyter Lab: http://localhost:8888   O token ser\u00e1 exibido no terminal ao iniciar os servi\u00e7os.</p> </li> <li> <p>MinIO (Painel Web): http://localhost:9000 </p> </li> <li>Usu\u00e1rio: <code>minioadmin</code> </li> <li>Senha: <code>minioadmin</code></li> </ul>"},{"location":"docs/execucao/#como-testar","title":"\ud83e\uddea Como testar","text":"<ol> <li>Acesse o Jupyter Lab</li> <li>Abra o notebook <code>notebooks/main.ipynb</code></li> <li> <p>Execute as c\u00e9lulas para:</p> </li> <li> <p>Criar tabelas com Apache Iceberg</p> </li> <li>Ingerir dados no MinIO</li> <li>Processar com Spark</li> <li>Persistir dados com versionamento no Lakehouse</li> </ol> <p>Dica</p> <p>Voc\u00ea pode modificar os dados de entrada ou transformar os scripts conforme sua necessidade para simula\u00e7\u00f5es mais avan\u00e7adas.</p>"},{"location":"docs/instalacao/","title":"Instala\u00e7\u00e3o","text":""},{"location":"docs/instalacao/#passo-1-clonar-projeto","title":"Passo 1 - Clonar projeto","text":"<p>Para rodar o projeto roda este comando e siga as instru\u00e7\u00f5es a baixo: <pre><code>git clone https://github.com/LuizZomer/implementacao_data_lakehouse.git\n</code></pre></p>"},{"location":"docs/instalacao/#passo-2-rodar-o-docker-compose","title":"Passo 2 - Rodar o docker compose","text":"<p>Para iniciar as aplica\u00e7\u00f5es necessarias para uso voc\u00ea ter\u00e1 que rodar este comando na raiz do projeto: <pre><code>docker compose up\n</code></pre> Este compose foi retirado diretamente da documenta\u00e7\u00e3o ofical, apenas sendo modificado esta parte no servi\u00e7o do spark-iceberg:</p> <pre><code>  ports:\n        - 7077:7077 &lt;-- adicionado esta exposi\u00e7\u00e3o de portas\n        - 8888:8888\n        - 8080:8080\n        - 10000:10000\n        - 10001:10001\n</code></pre>"},{"location":"docs/instalacao/#passo-3-entrar-no-jupiter-notebook","title":"Passo 3 - Entrar no jupiter notebook","text":"<p>Por meio da URL voc\u00ea ir\u00e1 acessar o jupiter notebook de forma interna no container com a url: <pre><code>http://localhost:8888/tree?\n</code></pre></p>"},{"location":"docs/instalacao/#passo-4-copiar-csv-para-o-notebook","title":"Passo 4 - Copiar csv para o notebook","text":"<p>Na raiz do projeto tem um arquivo csv, com a home do jupyter notebook aberta voc\u00ea s\u00f3 arrasta e solta o arquivo. </p>"},{"location":"docs/instalacao/#passo-4-rodar-o-codigo","title":"Passo 4 - Rodar o codigo","text":"<p>J\u00e1 dentro do jupiter notebook voc\u00ea criar\u00e1 um novo notebook no bot\u00e3o da parte de cima da tela, apartir dele poder\u00e1 rodar os codigos que est\u00e3o na main.ipynb, j\u00e1 est\u00e3o na ordem correta para funcionar e com coment\u00e1rios explicando seus funcionamento de forma detalhada.</p>"},{"location":"docs/referencias/","title":"Refer\u00eancia R\u00e1pida: Iceberg + Spark com Docker Compose","text":"<p>\ud83d\udd17 Documenta\u00e7\u00e3o oficial Apache Iceberg: iceberg.apache.org/spark-quickstart</p> <p>\ud83d\udd17 Documenta\u00e7\u00e3o oficial Docker: docs.docker.com</p> <p>\ud83d\udd17 Documenta\u00e7\u00e3o oficial PySpark: spark.apache.org</p>"},{"location":"docs/referencias/#autores","title":"\ud83d\udc64 Autores","text":"<p>Conhe\u00e7a quem participou da cria\u00e7\u00e3o deste projeto:</p>"},{"location":"docs/referencias/#luiz-felipe-zoomer","title":"Luiz Felipe Zoomer","text":"<ul> <li>\ud83d\udceb GitHub </li> </ul>"},{"location":"docs/referencias/#luiz-filipe-linhares","title":"Luiz Filipe Linhares","text":"<ul> <li>\ud83d\udceb GitHub </li> </ul>"},{"location":"docs/referencias/#willian-minatto","title":"Willian Minatto","text":"<ul> <li>\ud83d\udceb GitHub</li> </ul>"},{"location":"docs/tecnologias/","title":"Tecnologias","text":""},{"location":"docs/tecnologias/#tecnologias-usadas","title":"Tecnologias usadas:","text":"<ul> <li>UV (Gerenciador de pacotes python)</li> <li>Min.io (Data lake)</li> <li>Apache iceberg</li> <li>Apache spark</li> <li>Pyspark</li> <li>Jupyter notebook</li> <li>Docker</li> </ul>"}]}